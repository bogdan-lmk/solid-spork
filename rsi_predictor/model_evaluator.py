"""
–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
"""
import numpy as np
from typing import Dict
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

class ModelEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    
    @staticmethod
    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # RSI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        errors = np.abs(y_true - y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        accuracy_1 = np.mean(errors <= 1) * 100
        accuracy_2 = np.mean(errors <= 2) * 100
        accuracy_5 = np.mean(errors <= 5) * 100
        
        return {
            'mae': mae,
            'mse': mse,
            'r2': r2,
            'mape': mape,
            'accuracy_1': accuracy_1,
            'accuracy_2': accuracy_2,
            'accuracy_5': accuracy_5
        }
    
    @staticmethod
    def print_evaluation(train_metrics: Dict, test_metrics: Dict):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫"""
        print("\n" + "="*60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
        print("="*60)
        
        print(f"\nüìà –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<15} {'Train':<12} {'Test':<12} {'–†–∞–∑–Ω–æ—Å—Ç—å':<12}")
        print("-" * 55)
        print(f"{'MAE':<15} {train_metrics['mae']:<12.4f} {test_metrics['mae']:<12.4f} {abs(train_metrics['mae'] - test_metrics['mae']):<12.4f}")
        print(f"{'MSE':<15} {train_metrics['mse']:<12.4f} {test_metrics['mse']:<12.4f} {abs(train_metrics['mse'] - test_metrics['mse']):<12.4f}")
        print(f"{'R¬≤':<15} {train_metrics['r2']:<12.4f} {test_metrics['r2']:<12.4f} {abs(train_metrics['r2'] - test_metrics['r2']):<12.4f}")
        
        print(f"\nüéØ –¢–û–ß–ù–û–°–¢–¨ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø RSI (Test):")
        print(f"MAPE: {test_metrics['mape']:.2f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å ¬±1 –ø—É–Ω–∫—Ç:  {test_metrics['accuracy_1']:.1f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å ¬±2 –ø—É–Ω–∫—Ç–∞: {test_metrics['accuracy_2']:.1f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å ¬±5 –ø—É–Ω–∫—Ç–æ–≤: {test_metrics['accuracy_5']:.1f}%")
        
        # –û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        overfitting_score = abs(train_metrics['r2'] - test_metrics['r2'])
        if overfitting_score < 0.05:
            print(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: –ù–∏–∑–∫–æ–µ ({overfitting_score:.3f})")
        elif overfitting_score < 0.1:
            print(f"‚ö†Ô∏è  –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: –°—Ä–µ–¥–Ω–µ–µ ({overfitting_score:.3f})")
        else:
            print(f"‚ùå –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: –í—ã—Å–æ–∫–æ–µ ({overfitting_score:.3f})")