import pandas as pd
import numpy as np
import talib
from typing import Tuple, Dict, Optional, List, Union
from dataclasses import dataclass
from pathlib import Path
import logging
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings

# ONNX —ç–∫—Å–ø–æ—Ä—Ç
try:
    from skl2onnx import convert_sklearn
    from skl2onnx.common.data_types import FloatTensorType
    from onnxmltools import convert_catboost, convert_xgboost
    from onnxmltools.convert.common.data_types import FloatTensorType as OnnxFloatTensorType
    ONNX_AVAILABLE = True
except ImportError:
    ONNX_AVAILABLE = False
    logging.warning("ONNX –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install onnx onnxmltools skl2onnx")

warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
    model_type: str = 'catboost'
    test_size: float = 0.2
    cv_folds: int = 5
    random_state: int = 42
    
    # CatBoost –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    catboost_params: Dict = None
    
    # XGBoost –ø–∞—Ä–∞–º–µ—Ç—Ä—ã  
    xgboost_params: Dict = None
    
    def __post_init__(self):
        if self.catboost_params is None:
            self.catboost_params = {
                'iterations': 1000,
                'learning_rate': 0.05,
                'depth': 6,
                'random_seed': self.random_state,
                'verbose': False,
                'early_stopping_rounds': 50
            }
            
        if self.xgboost_params is None:
            self.xgboost_params = {
                'n_estimators': 1000,
                'learning_rate': 0.05,
                'max_depth': 6,
                'random_state': self.random_state,
                'early_stopping_rounds': 50
            }

@dataclass
class PredictionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    predicted_rsi: float
    current_rsi: float
    confidence: float
    change: float
    prediction_date: pd.Timestamp
    
    def __str__(self):
        return f"RSI: {self.current_rsi:.2f} ‚Üí {self.predicted_rsi:.2f} ({self.change:+.2f})"

class DataAdapter:
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ CSV –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def detect_format(df: pd.DataFrame) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö"""
        columns = set(df.columns.str.lower())
        
        if {'open', 'high', 'low', 'close'}.issubset(columns):
            return 'ohlcv'
        elif {'choppiness_index', 'volatility_percent', 'rsi_delta'}.issubset(columns):
            return 'indicators_only'
        elif 'close' in columns:
            return 'price_only'
        else:
            return 'unknown'
    
    @staticmethod
    def load_csv(filepath: str, **kwargs) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ CSV —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π"""
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            for sep in [',', ';', '\t']:
                try:
                    df = pd.read_csv(filepath, sep=sep, **kwargs)
                    if len(df.columns) > 1:  # –£—Å–ø–µ—à–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
                        logger.info(f"CSV –∑–∞–≥—Ä—É–∂–µ–Ω —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º '{sep}', –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
                        return df

class FeatureEngineer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—ã–Ω–µ—Å–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)"""
    
    @staticmethod
    def validate_data(df: pd.DataFrame) -> pd.DataFrame:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        required_columns = ['open', 'high', 'low', 'close']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        for col in required_columns:
            if df[col].isna().any():
                logger.warning(f"–ù–∞–π–¥–µ–Ω—ã NaN –≤ –∫–æ–ª–æ–Ω–∫–µ {col}, –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ—Ç–æ–¥–æ–º forward fill")
                df[col] = df[col].fillna(method='ffill')
        
        # –ï—Å–ª–∏ volume –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        if 'volume' not in df.columns:
            logger.info("Volume –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–º")
            df['volume'] = np.random.randint(1000000, 10000000, len(df))
            
        return df.copy()
    
    @staticmethod
    def create_rsi_features(df: pd.DataFrame, periods: List[int] = [14]) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ RSI –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        result = df.copy()
        
        for period in periods:
            rsi_col = f'rsi_{period}' if period != 14 else 'rsi'
            result[rsi_col] = talib.RSI(df['close'], timeperiod=period)
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ RSI
            result[f'{rsi_col}_sma_5'] = talib.SMA(result[rsi_col], timeperiod=5)
            result[f'{rsi_col}_ema_5'] = talib.EMA(result[rsi_col], timeperiod=5)
            result[f'{rsi_col}_change'] = result[rsi_col].diff()
            result[f'{rsi_col}_velocity'] = result[f'{rsi_col}_change'].diff()
            result[f'{rsi_col}_volatility'] = result[rsi_col].rolling(window=10).std()
            
            # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            for lag in [1, 2, 3, 5]:
                result[f'{rsi_col}_lag_{lag}'] = result[rsi_col].shift(lag)
        
        return result
    
    @staticmethod
    def create_oscillator_features(df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        result = df.copy()
        
        # Stochastic Oscillator
        result['stoch_k'], result['stoch_d'] = talib.STOCH(
            df['high'], df['low'], df['close'], 
            fastk_period=14, slowk_period=3, slowd_period=3
        )
        result['stoch_divergence'] = result['stoch_k'] - result['stoch_d']
        
        # Williams %R
        result['williams_r'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=14)
        
        # CCI
        result['cci'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)
        
        # MFI
        result['mfi'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=14)
        
        # Ultimate Oscillator
        result['ultimate_osc'] = talib.ULTOSC(df['high'], df['low'], df['close'])
        
        # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        result['momentum'] = talib.MOM(df['close'], timeperiod=10)
        result['roc'] = talib.ROC(df['close'], timeperiod=10)
        
        return result
    
    @staticmethod
    def create_trend_features(df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        result = df.copy()
        
        # MACD
        result['macd'], result['macd_signal'], result['macd_hist'] = talib.MACD(
            df['close'], fastperiod=12, slowperiod=26, signalperiod=9
        )
        result['macd_normalized'] = result['macd'] / df['close'] * 100
        
        # Bollinger Bands
        bb_upper, bb_middle, bb_lower = talib.BBANDS(df['close'], timeperiod=20)
        result['bb_percent_b'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
        result['bb_width'] = (bb_upper - bb_lower) / bb_middle
        
        # ADX
        result['adx_talib'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)
        
        return result
    
    @classmethod
    def create_all_features(cls, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        df = cls.validate_data(df)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = cls.create_rsi_features(df)
        df = cls.create_oscillator_features(df)
        df = cls.create_trend_features(df)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        df['rsi_next'] = df['rsi'].shift(-1)
        
        return df
                except:
                    continue
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø—è—Ç—É—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            df = pd.read_csv(filepath, **kwargs)
            logger.info(f"CSV –∑–∞–≥—Ä—É–∂–µ–Ω —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
            return df
            
        except Exception as e:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV —Ñ–∞–π–ª: {e}")
    
    @staticmethod
    def clean_accumulated_data(df: pd.DataFrame) -> pd.DataFrame:
        """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ accumulatedData —Ñ–∞–π–ª–æ–≤"""
        df = df.copy()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —á–∏—Å–ª–æ–≤—ã–µ
        numeric_columns = [
            'open', 'high', 'low', 'close', 'volume',
            'atr', 'atr_stop', 'atr_to_price_ratio',
            'fast_ema', 'slow_ema', 'ema_fast_deviation',
            'pchange', 'avpchange', 'gma', 'gma_smoothed',
            'positionBetweenBands', 'bollinger_position',
            'choppiness_index', 'volatility_percent',
            'rsi_volatility', 'adx', 'rsi_delta', 'linear_regression'
        ]
        
        for col in numeric_columns:
            if col in df.columns:
                try:
                    # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –Ω–∞ —Ç–æ—á–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    if df[col].dtype == 'object':
                        df[col] = df[col].astype(str).str.replace(',', '.')
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É {col}: {e}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
        if 'open_time' in df.columns:
            try:
                df['open_time'] = pd.to_datetime(df['open_time'])
                df = df.sort_values('open_time').reset_index(drop=True)
            except:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –∫—Ä–∏—Ç–∏—á–Ω—ã–º–∏ NaN (–≤ OHLC)
        critical_columns = ['open', 'high', 'low', 'close']
        before_rows = len(df)
        df = df.dropna(subset=critical_columns)
        after_rows = len(df)
        
        if before_rows != after_rows:
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {before_rows - after_rows} —Å—Ç—Ä–æ–∫ —Å NaN –≤ OHLC –¥–∞–Ω–Ω—ã—Ö")
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö NaN
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        return df
    
    @staticmethod
    def adapt_to_ohlcv(df: pd.DataFrame) -> pd.DataFrame:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∫ —Ñ–æ—Ä–º–∞—Ç—É OHLCV"""
        df = df.copy()
        format_type = DataAdapter.detect_format(df)
        
        if format_type == 'ohlcv':
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è—é—Ç—Å—è –ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ accumulatedData
            if 'open_time' in df.columns and 'atr' in df.columns:
                logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ accumulatedData - –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É")
                df = DataAdapter.clean_accumulated_data(df)
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ OHLCV
                required_cols = ['open', 'high', 'low', 'close']
                df = df.rename(columns={col: col.lower() for col in df.columns})
                
                missing = [col for col in required_cols if col not in df.columns]
                if missing:
                    raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing}")
                    
        elif format_type == 'price_only':
            # –¢–æ–ª—å–∫–æ —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è - —Å–æ–∑–¥–∞–µ–º OHLC
            if 'close' not in df.columns:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ 'close'")
            
            df['open'] = df['close'].shift(1).fillna(df['close'].iloc[0])
            df['high'] = df[['open', 'close']].max(axis=1) * (1 + np.random.uniform(0, 0.01, len(df)))
            df['low'] = df[['open', 'close']].min(axis=1) * (1 - np.random.uniform(0, 0.01, len(df)))
            
            logger.info("–°–æ–∑–¥–∞–Ω —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π OHLC –∏–∑ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è")
            
        elif format_type == 'indicators_only':
            raise ValueError("–î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –±–µ–∑ —Ü–µ–Ω. –ù—É–∂–Ω—ã OHLCV –¥–∞–Ω–Ω—ã–µ.")
        
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö. –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º volume –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if 'volume' not in df.columns:
            df['volume'] = np.random.randint(1000000, 10000000, len(df))
            logger.info("–î–æ–±–∞–≤–ª–µ–Ω —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–º —Ç–æ—Ä–≥–æ–≤")
        
        return df
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–≤—ã–Ω–µ—Å–µ–Ω –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)"""
    
    @staticmethod
    def validate_data(df: pd.DataFrame) -> pd.DataFrame:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        required_columns = ['open', 'high', 'low', 'close']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        for col in required_columns:
            if df[col].isna().any():
                logger.warning(f"–ù–∞–π–¥–µ–Ω—ã NaN –≤ –∫–æ–ª–æ–Ω–∫–µ {col}, –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ—Ç–æ–¥–æ–º forward fill")
                df[col] = df[col].fillna(method='ffill')
        
        # –ï—Å–ª–∏ volume –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        if 'volume' not in df.columns:
            logger.info("Volume –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–º")
            df['volume'] = np.random.randint(1000000, 10000000, len(df))
            
        return df.copy()
    
    @staticmethod
    def create_rsi_features(df: pd.DataFrame, periods: List[int] = [14]) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ RSI –∏ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        result = df.copy()
        
        for period in periods:
            rsi_col = f'rsi_{period}' if period != 14 else 'rsi'
            result[rsi_col] = talib.RSI(df['close'], timeperiod=period)
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ RSI
            result[f'{rsi_col}_sma_5'] = talib.SMA(result[rsi_col], timeperiod=5)
            result[f'{rsi_col}_ema_5'] = talib.EMA(result[rsi_col], timeperiod=5)
            result[f'{rsi_col}_change'] = result[rsi_col].diff()
            result[f'{rsi_col}_velocity'] = result[f'{rsi_col}_change'].diff()
            result[f'{rsi_col}_volatility'] = result[rsi_col].rolling(window=10).std()
            
            # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            for lag in [1, 2, 3, 5]:
                result[f'{rsi_col}_lag_{lag}'] = result[rsi_col].shift(lag)
        
        return result
    
    @staticmethod
    def create_oscillator_features(df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å—Ü–∏–ª–ª—è—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        result = df.copy()
        
        # Stochastic Oscillator
        result['stoch_k'], result['stoch_d'] = talib.STOCH(
            df['high'], df['low'], df['close'], 
            fastk_period=14, slowk_period=3, slowd_period=3
        )
        result['stoch_divergence'] = result['stoch_k'] - result['stoch_d']
        
        # Williams %R
        result['williams_r'] = talib.WILLR(df['high'], df['low'], df['close'], timeperiod=14)
        
        # CCI
        result['cci'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=14)
        
        # MFI
        result['mfi'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=14)
        
        # Ultimate Oscillator
        result['ultimate_osc'] = talib.ULTOSC(df['high'], df['low'], df['close'])
        
        # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        result['momentum'] = talib.MOM(df['close'], timeperiod=10)
        result['roc'] = talib.ROC(df['close'], timeperiod=10)
        
        return result
    
    @staticmethod
    def create_trend_features(df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        result = df.copy()
        
        # MACD
        result['macd'], result['macd_signal'], result['macd_hist'] = talib.MACD(
            df['close'], fastperiod=12, slowperiod=26, signalperiod=9
        )
        result['macd_normalized'] = result['macd'] / df['close'] * 100
        
        # Bollinger Bands
        bb_upper, bb_middle, bb_lower = talib.BBANDS(df['close'], timeperiod=20)
        result['bb_percent_b'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
        result['bb_width'] = (bb_upper - bb_lower) / bb_middle
        
        # ADX
        result['adx'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)
        
        return result
    
    @classmethod
    def create_all_features(cls, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        df = cls.validate_data(df)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = cls.create_rsi_features(df)
        df = cls.create_oscillator_features(df)
        df = cls.create_trend_features(df)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        df['rsi_next'] = df['rsi'].shift(-1)
        
        return df

class ModelEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    
    @staticmethod
    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # RSI-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        errors = np.abs(y_true - y_pred)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        accuracy_1 = np.mean(errors <= 1) * 100
        accuracy_2 = np.mean(errors <= 2) * 100
        accuracy_5 = np.mean(errors <= 5) * 100
        
        return {
            'mae': mae,
            'mse': mse,
            'r2': r2,
            'mape': mape,
            'accuracy_1': accuracy_1,
            'accuracy_2': accuracy_2,
            'accuracy_5': accuracy_5
        }
    
    @staticmethod
    def print_evaluation(train_metrics: Dict, test_metrics: Dict):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫"""
        print("\n" + "="*60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò")
        print("="*60)
        
        print(f"\nüìà –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<15} {'Train':<12} {'Test':<12} {'–†–∞–∑–Ω–æ—Å—Ç—å':<12}")
        print("-" * 55)
        print(f"{'MAE':<15} {train_metrics['mae']:<12.4f} {test_metrics['mae']:<12.4f} {abs(train_metrics['mae'] - test_metrics['mae']):<12.4f}")
        print(f"{'MSE':<15} {train_metrics['mse']:<12.4f} {test_metrics['mse']:<12.4f} {abs(train_metrics['mse'] - test_metrics['mse']):<12.4f}")
        print(f"{'R¬≤':<15} {train_metrics['r2']:<12.4f} {test_metrics['r2']:<12.4f} {abs(train_metrics['r2'] - test_metrics['r2']):<12.4f}")
        
        print(f"\nüéØ –¢–û–ß–ù–û–°–¢–¨ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø RSI (Test):")
        print(f"MAPE: {test_metrics['mape']:.2f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å ¬±1 –ø—É–Ω–∫—Ç:  {test_metrics['accuracy_1']:.1f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å ¬±2 –ø—É–Ω–∫—Ç–∞: {test_metrics['accuracy_2']:.1f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å ¬±5 –ø—É–Ω–∫—Ç–æ–≤: {test_metrics['accuracy_5']:.1f}%")
        
        # –û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        overfitting_score = abs(train_metrics['r2'] - test_metrics['r2'])
        if overfitting_score < 0.05:
            print(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: –ù–∏–∑–∫–æ–µ ({overfitting_score:.3f})")
        elif overfitting_score < 0.1:
            print(f"‚ö†Ô∏è  –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: –°—Ä–µ–¥–Ω–µ–µ ({overfitting_score:.3f})")
        else:
            print(f"‚ùå –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: –í—ã—Å–æ–∫–æ–µ ({overfitting_score:.3f})")

class RSIPredictor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä RSI —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ –ø—Ä–æ–µ–∫—Ç"""
    
    def __init__(self, config: Optional[ModelConfig] = None):
        self.config = config or ModelConfig()
        self.model = None
        self.scaler = RobustScaler()  # –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º
        self.feature_names: List[str] = []
        self.is_trained = False
        
    def _get_model(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if self.config.model_type == 'catboost':
            return CatBoostRegressor(**self.config.catboost_params)
        elif self.config.model_type == 'xgboost':
            return XGBRegressor(**self.config.xgboost_params)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {self.config.model_type}")
    
    def _prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏ –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        exclude_columns = {
            'open', 'high', 'low', 'close', 'volume', 'rsi_next', 'rsi'
        }
        
        feature_columns = [col for col in df.columns if col not in exclude_columns]
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN
        df_clean = df.dropna()
        
        if len(df_clean) == 0:
            raise ValueError("–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        X = df_clean[feature_columns]
        y = df_clean['rsi_next']
        
        self.feature_names = feature_columns
        return X, y
    
    def train(self, df: pd.DataFrame, save_path: Optional[str] = None) -> Dict[str, float]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        
        Args:
            df: DataFrame —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
            save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            
        Returns:
            Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df_features = FeatureEngineer.create_all_features(df)
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len([col for col in df_features.columns if col not in ['open', 'high', 'low', 'close', 'volume']])}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self._prepare_features(df_features)
        logger.info(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {X.shape}, —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {y.shape}")
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        split_idx = int(len(X) * (1 - self.config.test_size))
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        X_train_scaled = pd.DataFrame(
            self.scaler.fit_transform(X_train),
            columns=self.feature_names,
            index=X_train.index
        )
        X_test_scaled = pd.DataFrame(
            self.scaler.transform(X_test),
            columns=self.feature_names,
            index=X_test.index
        )
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = self._get_model()
        
        if self.config.model_type == 'catboost':
            self.model.fit(
                X_train_scaled, y_train,
                eval_set=(X_test_scaled, y_test),
                use_best_model=True
            )
        else:
            self.model.fit(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        y_pred_train = self.model.predict(X_train_scaled)
        y_pred_test = self.model.predict(X_test_scaled)
        
        train_metrics = ModelEvaluator.calculate_metrics(y_train, y_pred_train)
        test_metrics = ModelEvaluator.calculate_metrics(y_test, y_pred_test)
        
        ModelEvaluator.print_evaluation(train_metrics, test_metrics)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv_scores = self._cross_validate(X_train_scaled, y_train)
        logger.info(f"CV R¬≤ score: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
        
        self.is_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if save_path:
            self.save(save_path)
        
        return test_metrics
    
    def _cross_validate(self, X: pd.DataFrame, y: pd.Series) -> np.ndarray:
        """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
        tscv = TimeSeriesSplit(n_splits=self.config.cv_folds)
        cv_scores = []
        
        for train_idx, val_idx in tscv.split(X):
            X_cv_train, X_cv_val = X.iloc[train_idx], X.iloc[val_idx]
            y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]
            
            model = self._get_model()
            model.fit(X_cv_train, y_cv_train)
            
            y_cv_pred = model.predict(X_cv_val)
            score = r2_score(y_cv_val, y_cv_pred)
            cv_scores.append(score)
        
        return np.array(cv_scores)
    
    def predict(self, df: pd.DataFrame, return_confidence: bool = False) -> Union[float, PredictionResult]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ RSI –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            return_confidence: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            
        Returns:
            float –∏–ª–∏ PredictionResult
        """
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ train()")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df_features = FeatureEngineer.create_all_features(df)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, _ = self._prepare_features(df_features)
        
        if len(X) == 0:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–æ—Å—Ç—É–ø–Ω—É—é —Å—Ç—Ä–æ–∫—É
        X_last = X.iloc[[-1]]
        X_last_scaled = self.scaler.transform(X_last)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = self.model.predict(X_last_scaled)[0]
        prediction = np.clip(prediction, 0, 100)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º RSI
        
        if not return_confidence:
            return prediction
        
        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        current_rsi = df_features['rsi'].iloc[-1]
        change = prediction - current_rsi
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
        confidence = min(95.0, max(50.0, 100 - abs(change) * 5))
        
        return PredictionResult(
            predicted_rsi=prediction,
            current_rsi=current_rsi,
            confidence=confidence,
            change=change,
            prediction_date=pd.Timestamp.now()
        )
    
    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if not self.is_trained or not hasattr(self.model, 'feature_importances_'):
            return pd.DataFrame()
        
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        return importance_df.head(top_n)
    
    def plot_feature_importance(self, top_n: int = 20):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        importance_df = self.get_feature_importance(top_n)
        
        if importance_df.empty:
            logger.warning("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return
        
        plt.figure(figsize=(12, 8))
        sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')
        plt.title(f'–¢–æ–ø-{top_n} –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è RSI')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
        plt.tight_layout()
        plt.show()
    
    def save(self, filepath: str, export_onnx: bool = True):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ PKL –∏ ONNX —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ PKL
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'config': self.config,
            'is_trained': self.is_trained
        }
        
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(model_data, filepath)
        logger.info(f"PKL –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
        
        # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
        if export_onnx and ONNX_AVAILABLE and self.is_trained:
            try:
                onnx_path = filepath.replace('.pkl', '.onnx')
                self._export_onnx(onnx_path)
                logger.info(f"ONNX –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {onnx_path}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ ONNX: {e}")
        elif export_onnx and not ONNX_AVAILABLE:
            logger.warning("ONNX —ç–∫—Å–ø–æ—Ä—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.")
    
    def _export_onnx(self, onnx_path: str):
        """–≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ ONNX —Ñ–æ—Ä–º–∞—Ç"""
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        n_features = len(self.feature_names)
        initial_type = [('float_input', OnnxFloatTensorType([None, n_features]))]
        
        try:
            if self.config.model_type == 'catboost':
                # –≠–∫—Å–ø–æ—Ä—Ç CatBoost
                onnx_model = convert_catboost(self.model, initial_types=initial_type)
            elif self.config.model_type == 'xgboost':
                # –≠–∫—Å–ø–æ—Ä—Ç XGBoost
                onnx_model = convert_xgboost(self.model, initial_types=initial_type)
            else:
                raise ValueError(f"ONNX —ç–∫—Å–ø–æ—Ä—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è {self.config.model_type}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ONNX –º–æ–¥–µ–ª–∏
            with open(onnx_path, "wb") as f:
                f.write(onnx_model.SerializeToString())
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è ONNX (scaler –∏ feature names)
            metadata = {
                'feature_names': self.feature_names,
                'scaler_mean': self.scaler.center_.tolist() if hasattr(self.scaler, 'center_') else None,
                'scaler_scale': self.scaler.scale_.tolist() if hasattr(self.scaler, 'scale_') else None,
                'scaler_type': type(self.scaler).__name__
            }
            
            metadata_path = onnx_path.replace('.onnx', '_metadata.json')
            import json
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"ONNX –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ ONNX: {e}")
            raise
    
    def load(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.config = model_data['config']
        self.is_trained = model_data['is_trained']
        
        logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –ø—Ä–æ–µ–∫—Ç–æ–º
def integrate_with_existing_data(csv_path: str, **csv_kwargs) -> RSIPredictor:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–µ–∫—Ç–∞
    
    Args:
        csv_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
        **csv_kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pd.read_csv
        
    Returns:
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å RSIPredictor
    """
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {csv_path}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = DataAdapter.load_csv(csv_path, **csv_kwargs)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}, –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    logger.info(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    
    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ OHLCV —Ñ–æ—Ä–º–∞—Ç—É
    df_ohlcv = DataAdapter.adapt_to_ohlcv(df)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    config = ModelConfig(
        model_type='catboost',
        test_size=0.2,
        cv_folds=5
    )
    
    predictor = RSIPredictor(config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)
    
    # –û–±—É—á–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    model_path = model_dir / "rsi_predictor.pkl"
    metrics = predictor.train(df_ohlcv, save_path=str(model_path))
    
    logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    return predictor

def quick_predict_from_csv(csv_path: str, model_path: str = "models/rsi_predictor.pkl") -> PredictionResult:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ RSI –∏–∑ CSV —Ñ–∞–π–ª–∞
    
    Args:
        csv_path: –ü—É—Ç—å –∫ CSV —Å –¥–∞–Ω–Ω—ã–º–∏
        model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    predictor = RSIPredictor()
    predictor.load(model_path)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = DataAdapter.load_csv(csv_path)
    df_ohlcv = DataAdapter.adapt_to_ohlcv(df)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    result = predictor.predict(df_ohlcv, return_confidence=True)
    return result

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
def analyze_your_csv(csv_path: str):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CSV —Ñ–∞–π–ª–∞"""
    df = DataAdapter.load_csv(csv_path)
    
    print(f"\nüìä –ê–ù–ê–õ–ò–ó CSV –§–ê–ô–õ–ê: {csv_path}")
    print("="*60)
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏ ({len(df.columns)}): {list(df.columns)}")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
    format_type = DataAdapter.detect_format(df)
    print(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {format_type}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    try:
        for col in df.columns[:10]:  # –ü–µ—Ä–≤—ã–µ 10 –∫–æ–ª–æ–Ω–æ–∫
            non_null = df[col].count()
            null_count = len(df) - non_null
            dtype = str(df[col].dtype)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
            print(f"  {col:<25} | {dtype:<10} | Non-null: {non_null:<6} | Null: {null_count}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º
    if format_type in ['ohlcv', 'price_only']:
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º")
    elif format_type == 'indicators_only':
        if 'close' in df.columns:
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã (–µ—Å—Ç—å —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è)")
        else:
            print(f"‚ö†Ô∏è  –ù—É–∂–Ω–∞ —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞")
    else:
        print(f"‚ùå –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–±—É—é—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
    
    return df

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –≤–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
if __name__ == "__main__":
    # 1. –ê–ù–ê–õ–ò–ó –í–ê–®–ò–• –î–ê–ù–ù–´–•
    print("üîç –ê–Ω–∞–ª–∏–∑ –≤–∞—à–∏—Ö CSV —Ñ–∞–π–ª–æ–≤:")
    
    # –í–∞—à–∏ —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏
    your_csv_files = [
        "accumulatedData_2024.csv",
        "accumulatedData_2025.csv",
        "data.csv"  # –û—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    ]
    
    for csv_file in your_csv_files:
        try:
            print(f"\n--- –ê–Ω–∞–ª–∏–∑ {csv_file} ---")
            analyze_your_csv(csv_file)
        except FileNotFoundError:
            print(f"–§–∞–π–ª {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {csv_file}: {e}")
    
    # 2. –û–ë–£–ß–ï–ù–ò–ï –ù–ê –í–ê–®–ò–• –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–•
    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ 2024, –ø–æ—Ç–æ–º 2025
    priority_files = [
        "accumulatedData_2024.csv",
        "accumulatedData_2025.csv"
    ]
    
    trained_successfully = False
    
    for csv_file in priority_files:
        try:
            print(f"\nüìä –ü–æ–ø—ã—Ç–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {csv_file}...")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            config = ModelConfig(
                model_type='catboost',
                test_size=0.2,
                cv_folds=3,  # –ú–µ–Ω—å—à–µ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                catboost_params={
                    'iterations': 500,  # –ú–µ–Ω—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                    'learning_rate': 0.1,
                    'depth': 6,
                    'random_seed': 42,
                    'verbose': False,
                    'early_stopping_rounds': 50
                }
            )
            
            predictor = RSIPredictor(config)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            df = DataAdapter.load_csv(csv_file)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è accumulatedData
            if 'open_time' in df.columns:
                print(f"üìÖ –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {df['open_time'].iloc[0]} - {df['open_time'].iloc[-1]}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            model_path = f"models/rsi_predictor_{csv_file.replace('.csv', '')}.pkl"
            metrics = predictor.train(df, save_path=model_path)
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ {csv_file}")
            print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {model_path}")
            
            # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            result = predictor.predict(df, return_confidence=True)
            print(f"üîÆ –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result}")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            print(f"\nüìä –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            importance_df = predictor.get_feature_importance(10)
            for idx, row in importance_df.iterrows():
                print(f"  {row['feature']:<25} - {row['importance']:.4f}")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            try:
                predictor.plot_feature_importance(top_n=15)
            except:
                print("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            
            trained_successfully = True
            break
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {csv_file}: {e}")
            import traceback
            print(f"–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:\n{traceback.format_exc()}")
            continue
    
    if not trained_successfully:
        print(f"\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"üîß –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ fallback
        np.random.seed(42)
        dates = pd.date_range('2020-01-01', periods=500, freq='D')
        
        price_base = 100
        prices = [price_base]
        
        for i in range(499):
            trend = 0.0001 * i
            volatility = np.random.normal(0, 0.02)
            new_price = prices[-1] * (1 + trend + volatility)
            prices.append(max(new_price, 1))
        
        df = pd.DataFrame({
            'open_time': dates,
            'open': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
            'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'close': prices,
            'volume': np.random.randint(1000000, 10000000, 500)
        })
        
        config = ModelConfig(model_type='catboost', test_size=0.2, cv_folds=3)
        predictor = RSIPredictor(config)
        
        metrics = predictor.train(df, save_path="models/rsi_predictor_fallback.pkl")
        result = predictor.predict(df, return_confidence=True)
        
        print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
        print(f"üîÆ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")

# –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
def train_on_accumulated_data(file_2024: str = "accumulatedData_2024.csv", 
                             file_2025: str = "accumulatedData_2025.csv") -> RSIPredictor:
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞—à–∏—Ö accumulated –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        file_2024: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º 2024 –≥–æ–¥–∞
        file_2025: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º 2025 –≥–æ–¥–∞
        
    Returns:
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_2024 = DataAdapter.load_csv(file_2024)
    df_2025 = DataAdapter.load_csv(file_2025)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    df_combined = pd.concat([df_2024, df_2025], ignore_index=True)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if 'open_time' in df_combined.columns:
        df_combined['open_time'] = pd.to_datetime(df_combined['open_time'])
        df_combined = df_combined.sort_values('open_time').reset_index(drop=True)
    
    print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {df_combined.shape}")
    print(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {df_combined['open_time'].iloc[0]} - {df_combined['open_time'].iloc[-1]}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
    config = ModelConfig(
        model_type='catboost',
        test_size=0.15,  # –ú–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è
        cv_folds=5,
        catboost_params={
            'iterations': 1000,
            'learning_rate': 0.05,
            'depth': 8,  # –ë–æ–ª—å—à–µ –≥–ª—É–±–∏–Ω–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            'random_seed': 42,
            'verbose': 100,
            'early_stopping_rounds': 100
        }
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    predictor = RSIPredictor(config)
    metrics = predictor.train(df_combined, save_path="models/rsi_predictor_combined.pkl")
    
    return predictor

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def batch_predict_rsi(csv_directory: str, model_path: str = "models/rsi_predictor.pkl"):
    """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ RSI –¥–ª—è –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    results = {}
    
    for csv_file in Path(csv_directory).glob("*.csv"):
        try:
            result = quick_predict_from_csv(str(csv_file), model_path)
            results[csv_file.name] = result
            print(f"‚úÖ {csv_file.name}: {result}")
        except Exception as e:
            print(f"‚ùå {csv_file.name}: {e}")
    
    return results

def export_prediction_to_csv(prediction_result: PredictionResult, output_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ CSV"""
    df = pd.DataFrame([{
        'prediction_date': prediction_result.prediction_date,
        'current_rsi': prediction_result.current_rsi,
        'predicted_rsi': prediction_result.predicted_rsi,
        'change': prediction_result.change,
        'confidence': prediction_result.confidence
    }])
    
    df.to_csv(output_path, index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")