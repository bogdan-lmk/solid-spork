"""
–£—Ç–∏–ª–∏—Ç—ã –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
"""
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from config import ModelConfig
from data_adapter import DataAdapter
from rsi_predictor import RSIPredictor
from data_types import PredictionResult

logger = logging.getLogger(__name__)

def integrate_with_existing_data(csv_path: str, **csv_kwargs) -> RSIPredictor:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–µ–∫—Ç–∞
    
    Args:
        csv_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
        **csv_kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è pd.read_csv
        
    Returns:
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å RSIPredictor
    """
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {csv_path}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = DataAdapter.load_csv(csv_path, **csv_kwargs)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}, –∫–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    logger.info(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    
    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ OHLCV —Ñ–æ—Ä–º–∞—Ç—É
    df_ohlcv = DataAdapter.adapt_to_ohlcv(df)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    config = ModelConfig(
        model_type='catboost',
        test_size=0.2,
        cv_folds=5
    )
    
    predictor = RSIPredictor(config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)
    
    # –û–±—É—á–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    model_path = model_dir / "rsi_predictor.pkl"
    metrics = predictor.train(df_ohlcv, save_path=str(model_path))
    
    logger.info("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    return predictor

def quick_predict_from_csv(csv_path: str, model_path: str = "models/rsi_predictor.pkl") -> PredictionResult:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ RSI –∏–∑ CSV —Ñ–∞–π–ª–∞
    
    Args:
        csv_path: –ü—É—Ç—å –∫ CSV —Å –¥–∞–Ω–Ω—ã–º–∏
        model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    predictor = RSIPredictor()
    predictor.load(model_path)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = DataAdapter.load_csv(csv_path)
    df_ohlcv = DataAdapter.adapt_to_ohlcv(df)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    result = predictor.predict(df_ohlcv, return_confidence=True)
    return result

def analyze_your_csv(csv_path: str):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã CSV —Ñ–∞–π–ª–∞"""
    df = DataAdapter.load_csv(csv_path)
    
    print(f"\nüìä –ê–ù–ê–õ–ò–ó CSV –§–ê–ô–õ–ê: {csv_path}")
    print("="*60)
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏ ({len(df.columns)}): {list(df.columns)}")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
    format_type = DataAdapter.detect_format(df)
    print(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {format_type}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    try:
        for col in df.columns[:10]:  # –ü–µ—Ä–≤—ã–µ 10 –∫–æ–ª–æ–Ω–æ–∫
            non_null = df[col].count()
            null_count = len(df) - non_null
            dtype = str(df[col].dtype)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
            print(f"  {col:<25} | {dtype:<10} | Non-null: {non_null:<6} | Null: {null_count}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º
    if format_type in ['ohlcv', 'price_only']:
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã —Å RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º")
    elif format_type == 'indicators_only':
        if 'close' in df.columns:
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã (–µ—Å—Ç—å —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è)")
        else:
            print(f"‚ö†Ô∏è  –ù—É–∂–Ω–∞ —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞")
    else:
        print(f"‚ùå –î–∞–Ω–Ω—ã–µ —Ç—Ä–µ–±—É—é—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
    
    return df

def train_on_accumulated_data(file_2024: str = "accumulatedData_2024.csv", 
                             file_2025: str = "accumulatedData_2025.csv") -> RSIPredictor:
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞—à–∏—Ö accumulated –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        file_2024: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º 2024 –≥–æ–¥–∞
        file_2025: –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º 2025 –≥–æ–¥–∞
        
    Returns:
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df_2024 = DataAdapter.load_csv(file_2024)
    df_2025 = DataAdapter.load_csv(file_2025)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    df_combined = pd.concat([df_2024, df_2025], ignore_index=True)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if 'open_time' in df_combined.columns:
        df_combined['open_time'] = pd.to_datetime(df_combined['open_time'])
        df_combined = df_combined.sort_values('open_time').reset_index(drop=True)
    
    print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {df_combined.shape}")
    print(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {df_combined['open_time'].iloc[0]} - {df_combined['open_time'].iloc[-1]}")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
    config = ModelConfig(
        model_type='catboost',
        test_size=0.15,  # –ú–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è
        cv_folds=5,
        catboost_params={
            'iterations': 1000,
            'learning_rate': 0.05,
            'depth': 8,  # –ë–æ–ª—å—à–µ –≥–ª—É–±–∏–Ω–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            'random_seed': 42,
            'verbose': 100,
            'early_stopping_rounds': 100
        }
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    predictor = RSIPredictor(config)
    metrics = predictor.train(df_combined, save_path="models/rsi_predictor_combined.pkl")
    
    return predictor

def batch_predict_rsi(csv_directory: str, model_path: str = "models/rsi_predictor.pkl"):
    """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ RSI –¥–ª—è –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    results = {}
    
    for csv_file in Path(csv_directory).glob("*.csv"):
        try:
            result = quick_predict_from_csv(str(csv_file), model_path)
            results[csv_file.name] = result
            print(f"‚úÖ {csv_file.name}: {result}")
        except Exception as e:
            print(f"‚ùå {csv_file.name}: {e}")
    
    return results

def export_prediction_to_csv(prediction_result: PredictionResult, output_path: str):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ CSV"""
    df = pd.DataFrame([{
        'prediction_date': prediction_result.prediction_date,
        'current_rsi': prediction_result.current_rsi,
        'predicted_rsi': prediction_result.predicted_rsi,
        'change': prediction_result.change,
        'confidence': prediction_result.confidence
    }])
    
    df.to_csv(output_path, index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")

def create_test_data(n_rows: int = 500) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=n_rows, freq='D')
    
    price_base = 100
    prices = [price_base]
    
    for i in range(n_rows - 1):
        trend = 0.0001 * i
        volatility = np.random.normal(0, 0.02)
        new_price = prices[-1] * (1 + trend + volatility)
        prices.append(max(new_price, 1))
    
    df = pd.DataFrame({
        'open_time': dates,
        'open': [p * (1 + np.random.normal(0, 0.005)) for p in prices],
        'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
        'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
        'close': prices,
        'volume': np.random.randint(1000000, 10000000, n_rows)
    })
    
    return df
