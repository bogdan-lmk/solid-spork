"""
–ì–ª–∞–≤–Ω—ã–π –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª –¥–ª—è RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ (–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
"""
import sys
import os
from pathlib import Path
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path (absolute path)
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

import logging
import warnings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

def combine_all_csv_files(data_folder: Path):
    """
    –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ data
    
    Args:
        data_folder: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å CSV —Ñ–∞–π–ª–∞–º–∏
        
    Returns:
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame
    """
    from data_adapter import DataAdapter
    
    all_dataframes = []
    processed_files = []
    
    print(f"\nüîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏: {data_folder}")
    
    # –ù–∞–π—Ç–∏ –≤—Å–µ CSV —Ñ–∞–π–ª—ã
    csv_files = list(data_folder.glob("*.csv"))
    
    if not csv_files:
        return None, []
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤: {len(csv_files)}")
    
    for csv_file in csv_files:
        try:
            print(f"  üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {csv_file.name}...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
            df = DataAdapter.load_csv(str(csv_file))
            
            # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞
            format_type = DataAdapter.detect_format(df)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if format_type in ['ohlcv', 'price_only']:
                # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ OHLCV —Ñ–æ—Ä–º–∞—Ç—É
                df_adapted = DataAdapter.adapt_to_ohlcv(df)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö
                df_adapted['data_source'] = csv_file.stem
                
                all_dataframes.append(df_adapted)
                processed_files.append(csv_file.name)
                print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(df_adapted)} —Å—Ç—Ä–æ–∫")
                
            elif format_type == 'indicators_only':
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –±–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                if 'close' in df.columns or any(col in df.columns for col in ['open', 'high', 'low']):
                    df_adapted = DataAdapter.adapt_to_ohlcv(df)
                    df_adapted['data_source'] = csv_file.stem
                    all_dataframes.append(df_adapted)
                    processed_files.append(csv_file.name)
                    print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {len(df_adapted)} —Å—Ç—Ä–æ–∫")
                else:
                    print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω (–Ω–µ—Ç —Ü–µ–Ω): {csv_file.name}")
            else:
                print(f"  ‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {csv_file.name}")
                
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {csv_file.name}: {e}")
            continue
    
    if not all_dataframes:
        return None, []
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö DataFrame
    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞)
    if 'open_time' in combined_df.columns:
        try:
            combined_df['open_time'] = pd.to_datetime(combined_df['open_time'], errors='coerce')
            combined_df = combined_df.sort_values('open_time').reset_index(drop=True)
            print(f"  üìÖ –î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
        except Exception as e:
            print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {e}")
    
    print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
    print(f"   üìä –ò—Ç–æ–≥–æ —Å—Ç—Ä–æ–∫: {len(combined_df)}")
    print(f"   üìÅ –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_files)}")
    print(f"   üìã –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(processed_files)}")
    
    return combined_df, processed_files

def train_on_combined_data(combined_df, data_source_info):
    """
    –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    from config import ModelConfig
    from rsi_predictor import RSIPredictor
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    config = ModelConfig(
        model_type='catboost',
        test_size=0.15,  # –ú–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è
        cv_folds=5,
        catboost_params={
            'iterations': 1500,  # –ë–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            'learning_rate': 0.03,  # –ú–µ–Ω—å—à–µ learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            'depth': 6,
            'random_seed': 42,
            'verbose': 100,
            'early_stopping_rounds': 150,
            'l2_leaf_reg': 3,  # –ë–æ–ª—å—à–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        }
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    predictor = RSIPredictor(config)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
    models_dir = current_dir / "models"
    models_dir.mkdir(exist_ok=True)
    
    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(combined_df)} —Å—Ç—Ä–æ–∫")
    print(f"üìÅ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(data_source_info)}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    model_path = models_dir / "rsi_predictor_combined_data.pkl"
    metrics = predictor.train(combined_df, save_path=str(model_path))
    
    return predictor

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)"""
    
    print("üöÄ RSI Predictor - –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è RSI")
    print("=" * 60)
    
    try:
        # –ò–º–ø–æ—Ä—Ç—ã –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —Ü–∏—Ä–∫—É–ª—è—Ä–Ω—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
        from config import ModelConfig
        from rsi_predictor import RSIPredictor
        from data_adapter import DataAdapter
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å utilities —Å —Ä–∞–∑–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
        try:
            from utilities import (
                analyze_your_csv, 
                train_on_accumulated_data, 
                integrate_with_existing_data,
                create_test_data
            )
        except ImportError:
            # –ï—Å–ª–∏ utilities —Å –ø—Ä–æ–±–µ–ª–æ–º –≤ –∫–æ–Ω—Ü–µ
            import importlib.util
            utilities_path = current_dir / "utilities.py "
            if utilities_path.exists():
                spec = importlib.util.spec_from_file_location("utilities", utilities_path)
                utilities_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(utilities_module)
                
                analyze_your_csv = utilities_module.analyze_your_csv
                train_on_accumulated_data = utilities_module.train_on_accumulated_data
                integrate_with_existing_data = utilities_module.integrate_with_existing_data
                create_test_data = utilities_module.create_test_data
            else:
                raise ImportError("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –º–æ–¥—É–ª—å utilities")
        
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        print(f"üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ: {current_dir}")
        print(f"üìÅ –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
        
        # –ü—Ä–æ–≤–µ—Ä–∏–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        required_files = [
            "config.py",
            "rsi_predictor.py", 
            "data_adapter.py",
            "utilities.py",
            "utilities.py ",  # —Å –ø—Ä–æ–±–µ–ª–æ–º
            "feature_engineer.py",
            "model_evaluator.py",
            "data_types.py"
        ]
        
        print("\nüìã –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤:")
        for file in required_files:
            file_path = current_dir / file
            exists = "‚úÖ" if file_path.exists() else "‚ùå"
            print(f"{exists} {file}")
        
        return
    
    # –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–ø–∫–∏ data
    data_folder = current_dir / "data"
    use_data_folder = False
    
    if data_folder.exists() and list(data_folder.glob("*.csv")):
        print(f"\nüìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–∞–ø–∫–∞ data —Å CSV —Ñ–∞–π–ª–∞–º–∏!")
        csv_in_data = list(data_folder.glob("*.csv"))
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ data/: {len(csv_in_data)}")
        
        for csv_file in csv_in_data:
            print(f"  üìÑ {csv_file.name}")
        
        use_data_folder = True
        
        # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ data
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ data:")
        for csv_file in csv_in_data:
            try:
                print(f"\n--- –ê–Ω–∞–ª–∏–∑ {csv_file.name} ---")
                analyze_your_csv(str(csv_file))
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {csv_file.name}: {e}")
    else:
        print(f"\nüìÅ –ü–∞–ø–∫–∞ data –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –ø—É—Å—Ç–∞")
        print(f"üí° –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É data –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ CSV —Ñ–∞–π–ª—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    # 1. –ê–ù–ê–õ–ò–ó –í–ê–®–ò–• –î–ê–ù–ù–´–• (—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ –∫–∞–∫ fallback)
    if not use_data_folder:
        print("\nüîç –ê–Ω–∞–ª–∏–∑ CSV —Ñ–∞–π–ª–æ–≤ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö:")
        
        # –ü–æ–∏—Å–∫ CSV —Ñ–∞–π–ª–æ–≤ –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π
        csv_search_paths = [
            current_dir,  # –¢–µ–∫—É—â–∞—è –ø–∞–ø–∫–∞
            current_dir.parent,  # –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è –ø–∞–ø–∫–∞
            Path.cwd()  # –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        ]
        
        found_files = []
        target_files = [
            "accumulatedData_2024.csv",
            "accumulatedData_2025.csv",
            "data.csv"
        ]
        
        for search_path in csv_search_paths:
            for target_file in target_files:
                file_path = search_path / target_file
                if file_path.exists():
                    found_files.append(str(file_path))
                    print(f"üìÅ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {file_path}")
        
        if not found_files:
            print("‚ö†Ô∏è CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö")
            print("üîç –ò—â–µ–º –ª—é–±—ã–µ CSV —Ñ–∞–π–ª—ã...")
            
            # –ò—â–µ–º –ª—é–±—ã–µ CSV —Ñ–∞–π–ª—ã
            for search_path in csv_search_paths:
                csv_files = list(search_path.glob("*.csv"))
                if csv_files:
                    print(f"üìÅ –ù–∞–π–¥–µ–Ω—ã CSV —Ñ–∞–π–ª—ã –≤ {search_path}:")
                    for csv_file in csv_files[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                        print(f"  - {csv_file.name}")
                        found_files.append(str(csv_file))
                    break
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for csv_file in found_files[:3]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3
            try:
                print(f"\n--- –ê–Ω–∞–ª–∏–∑ {Path(csv_file).name} ---")
                analyze_your_csv(csv_file)
            except FileNotFoundError:
                print(f"–§–∞–π–ª {csv_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {csv_file}: {e}")
    
    # 2. –û–ë–£–ß–ï–ù–ò–ï –ù–ê –î–ê–ù–ù–´–•
    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞...")
    
    trained_successfully = False
    predictor = None
    
    # –ù–û–í–û–ï: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–∞–ø–∫–µ data
    if use_data_folder:
        try:
            print(f"\nüéØ –ü–†–ò–û–†–ò–¢–ï–¢: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö –∏–∑ –ø–∞–ø–∫–∏ data...")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ data
            combined_df, processed_files = combine_all_csv_files(data_folder)
            
            if combined_df is not None and len(combined_df) > 50:
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                predictor = train_on_combined_data(combined_df, processed_files)
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
                
                # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                result = predictor.predict(combined_df, return_confidence=True)
                print(f"üîÆ –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result}")
                
                # –ü–æ–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                print(f"\nüìä –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                importance_df = predictor.get_feature_importance(10)
                if not importance_df.empty:
                    for idx, row in importance_df.iterrows():
                        print(f"  {row['feature']:<25} - {row['importance']:.4f}")
                
                trained_successfully = True
            else:
                print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–ø–∫–µ data –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–∞–ø–∫–∏ data: {e}")
            import traceback
            print(traceback.format_exc())
    
    # –°–£–©–ï–°–¢–í–£–Æ–©–ê–Ø –õ–û–ì–ò–ö–ê –∫–∞–∫ fallback
    if not trained_successfully:
        print(f"\nüìä Fallback: –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        files_to_try = []
        
        if use_data_folder:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–∞–ø–∫–∞ data, –Ω–æ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫
            files_to_try = [str(f) for f in data_folder.glob("*.csv")]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ª–æ–≥–∏–∫–∏
            files_to_try = found_files[:2] if 'found_files' in locals() else []
        
        # –ü—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å—Å—è –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
        for csv_file in files_to_try:
            try:
                print(f"\nüìä –ü–æ–ø—ã—Ç–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {Path(csv_file).name}...")
                
                # –û–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
                predictor = integrate_with_existing_data(csv_file)
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ {Path(csv_file).name}")
                
                # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                df = DataAdapter.load_csv(csv_file)
                result = predictor.predict(df, return_confidence=True)
                print(f"üîÆ –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result}")
                
                # –ü–æ–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                print(f"\nüìä –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
                importance_df = predictor.get_feature_importance(10)
                if not importance_df.empty:
                    for idx, row in importance_df.iterrows():
                        print(f"  {row['feature']:<25} - {row['importance']:.4f}")
                
                trained_successfully = True
                break
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {Path(csv_file).name}: {e}")
                continue
    
    # 3. FALLBACK - —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞)
    if not trained_successfully:
        print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"üîß –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å...")
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ fallback
            df = create_test_data(500)
            
            config = ModelConfig(model_type='catboost', test_size=0.2, cv_folds=3)
            predictor = RSIPredictor(config)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
            models_dir = current_dir / "models"
            models_dir.mkdir(exist_ok=True)
            
            metrics = predictor.train(df, save_path=str(models_dir / "rsi_predictor_fallback.pkl"))
            result = predictor.predict(df, return_confidence=True)
            
            print(f"‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
            print(f"üîÆ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            trained_successfully = True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            print(traceback.format_exc())
    
    # 4. –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –†–ï–ó–£–õ–¨–¢–ê–¢–ê–•
    if trained_successfully:
        models_dir = current_dir / "models"
        if models_dir.exists():
            model_files = list(models_dir.glob("*.pkl"))
            print(f"\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ {models_dir}:")
            for model_file in model_files:
                print(f"  üìÑ {model_file.name}")
    
    print(f"\nüéâ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    # –ù–û–í–û–ï: –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"\nüìã –ü–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞: {current_dir}")
    if data_folder.exists():
        csv_count = len(list(data_folder.glob("*.csv")))
        print(f"üìä –ü–∞–ø–∫–∞ data: {csv_count} CSV —Ñ–∞–π–ª–æ–≤")
    else:
        print(f"üí° –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É data –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ç—É–¥–∞ CSV —Ñ–∞–π–ª—ã")
        print(f"   mkdir data")
        print(f"   mv *.csv data/")
    
    print(f"üêç Python path: {sys.path[0]}")
    print(f"üíæ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")

if __name__ == "__main__":
    main()
