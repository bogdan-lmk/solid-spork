"""
–ì–ª–∞–≤–Ω—ã–π –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª –¥–ª—è RSI –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–æ–π –∏–∑ –ø–∞–ø–∫–∏ data
"""
import sys
import os
from pathlib import Path
import pandas as pd
import glob
from data_adapter import DataAdapter


# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
current_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(current_dir))

import logging
import warnings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

def find_all_data_files():
    """
    –ü–æ–∏—Å–∫ –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ data –∏ –¥—Ä—É–≥–∏—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
    """
    data_files = []
    
    # –í–æ–∑–º–æ–∂–Ω—ã–µ –ø–∞–ø–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
    data_directories = [
        current_dir / "data",           # ./data/
        current_dir.parent / "data",    # ../data/
        current_dir,                    # —Ç–µ–∫—É—â–∞—è –ø–∞–ø–∫–∞
        Path.cwd() / "data",           # —Ä–∞–±–æ—á–∞—è_–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è/data/
        Path.cwd()                      # —Ä–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
    ]
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
    file_patterns = [
        "accumulatedData_*.csv",     # –í–∞—à–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        "*.csv"                     # –õ—é–±—ã–µ CSV (–±—É–¥–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–º)
    ]
    
    print("üîç –ü–æ–∏—Å–∫ CSV —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    found_files_info = []
    
    for data_dir in data_directories:
        if not data_dir.exists():
            continue
            
        print(f"üìÅ –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É: {data_dir}")
        
        for pattern in file_patterns:
            pattern_files = list(data_dir.glob(pattern))
            
            for file_path in pattern_files:
                if file_path.is_file() and file_path.suffix.lower() == '.csv':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –ª–∏ —É–∂–µ —ç—Ç–æ—Ç —Ñ–∞–π–ª
                    if not any(existing['path'].resolve() == file_path.resolve() for existing in found_files_info):
                        
                        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                        file_size = file_path.stat().st_size
                        file_size_mb = file_size / (1024 * 1024)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—É—Å—Ç–æ–π –ª–∏ —Ñ–∞–π–ª
                        if file_size > 1024:  # –ú–∏–Ω–∏–º—É–º 1KB
                            found_files_info.append({
                                'path': file_path,
                                'name': file_path.name,
                                'size_mb': file_size_mb,
                                'pattern': pattern,
                                'directory': data_dir
                            })
                            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω: {file_path.name} ({file_size_mb:.2f} MB)")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ —Ä–∞–∑–º–µ—Ä—É
    priority_order = {
        "accumulatedData_*.csv": 1,
        "*.csv": 2
    }
    
    found_files_info.sort(key=lambda x: (
        priority_order.get(x['pattern'], 99), 
        -x['size_mb']  # –ë–æ–ª—å—à–µ —Ñ–∞–π–ª—ã –ø–µ—Ä–≤—ã–º–∏
    ))
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    data_files = [info['path'] for info in found_files_info]
    
    print(f"\nüìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(data_files)}")
    
    if found_files_info:
        print("üìã –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
        for i, info in enumerate(found_files_info[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            print(f"  {i}. {info['name']} ({info['size_mb']:.2f} MB) - {info['pattern']}")
        
        if len(found_files_info) > 10:
            print(f"  ... –∏ –µ—â–µ {len(found_files_info) - 10} —Ñ–∞–π–ª–æ–≤")
    
    return data_files

def load_and_combine_data_files(data_files, max_files=5):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö CSV —Ñ–∞–π–ª–æ–≤
    """
    if not data_files:
        return None
    
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–º–∞–∫—Å. {max_files} —Ñ–∞–π–ª–æ–≤)...")
    
    # –ò–º–ø–æ—Ä—Ç—ã
    from data_adapter import DataAdapter
    
    combined_data = []
    successful_files = []
    
    for i, file_path in enumerate(data_files[:max_files]):
        try:
            print(f"\nüìñ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª {i+1}/{min(len(data_files), max_files)}: {file_path.name}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
            df = DataAdapter.load_csv(str(file_path))
            print(f"  –†–∞–∑–º–µ—Ä: {df.shape}")
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
            required_cols = ['open', 'high', 'low', 'close']
            has_ohlc = all(col in df.columns for col in required_cols)
            has_rsi_volatility = 'rsi_volatility' in df.columns
            
            print(f"  OHLC –¥–∞–Ω–Ω—ã–µ: {'‚úÖ' if has_ohlc else '‚ùå'}")
            print(f"  RSI volatility: {'‚úÖ' if has_rsi_volatility else '‚ùå'}")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not has_ohlc and not has_rsi_volatility:
                print(f"  ‚ö†Ô∏è –§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
            df['data_source'] = file_path.name
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            if 'open_time' in df.columns:
                try:
                    df['open_time'] = pd.to_datetime(df['open_time'], errors='coerce')
                    valid_dates = df['open_time'].notna().sum()
                    print(f"  –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {valid_dates}/{len(df)} –≤–∞–ª–∏–¥–Ω—ã—Ö")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏: {e}")
            
            combined_data.append(df)
            successful_files.append(file_path.name)
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path.name}: {e}")
            continue
    
    if not combined_data:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞!")
        return None
    
    print(f"\nüîó –û–±—ä–µ–¥–∏–Ω—è–µ–º {len(combined_data)} —Ñ–∞–π–ª–æ–≤...")
    
    try:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        df_combined = pd.concat(combined_data, ignore_index=True, sort=False)
        
        print(f"üìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {df_combined.shape}")
        print(f"üìÅ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã: {', '.join(successful_files)}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
        if 'open_time' in df_combined.columns:
            valid_time_mask = df_combined['open_time'].notna()
            if valid_time_mask.any():
                df_combined = df_combined.sort_values('open_time').reset_index(drop=True)
                
                time_range = df_combined.loc[valid_time_mask, 'open_time']
                print(f"üóìÔ∏è –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {time_range.min()} ‚Äî {time_range.max()}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        if 'open_time' in df_combined.columns:
            duplicates = df_combined.duplicated(subset=['open_time'], keep='first').sum()
            if duplicates > 0:
                print(f"üîÑ –£–¥–∞–ª—è–µ–º {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
                df_combined = df_combined.drop_duplicates(subset=['open_time'], keep='first')
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df_combined.shape}")
        print(f"üìà –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {df_combined['data_source'].nunique()}")
        
        return df_combined
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
        return combined_data[0] if len(combined_data) == 1 else None

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–æ–π"""
    
    print("üöÄ RSI Predictor - –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê —Å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–æ–π –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 70)
    
    try:
        # –ò–º–ø–æ—Ä—Ç—ã
        from config import ModelConfig
        from rsi_predictor import RSIPredictor
        from data_adapter import DataAdapter
        from utilities import analyze_your_csv, create_test_data
        
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        return
    
    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    data_files = find_all_data_files()
    
    if not data_files:
        print("\n‚ùå CSV —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        print("üí° –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É 'data' –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ –≤ –Ω–µ—ë CSV —Ñ–∞–π–ª—ã")
        print("üí° –ò–ª–∏ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç–µ CSV —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ fallback
        print("\nüîß –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...")
        try:
            df = create_test_data(500)
            # –î–æ–±–∞–≤–ª—è–µ–º rsi_volatility –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            import talib
            df['rsi_volatility'] = talib.RSI(df['close'], timeperiod=14)
            data_files = [df]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º DataFrame –Ω–∞–ø—Ä—è–º—É—é
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return
    else:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        df = load_and_combine_data_files(data_files, max_files=5)
        
        if df is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
            return
        
        data_files = [df]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π DataFrame
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìã –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•:")
    for i, data_file in enumerate(data_files):
        try:
            if isinstance(data_file, str):
                print(f"\n--- –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ {Path(data_file).name} ---")
                analyze_your_csv(data_file)
            else:
                print(f"\n--- –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
                print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {data_file.shape}")
                print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(data_file.columns)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
                if 'rsi_volatility' in data_file.columns:
                    rsi_data = data_file['rsi_volatility'].dropna()
                    print(f"RSI volatility: {len(rsi_data)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –¥–∏–∞–ø–∞–∑–æ–Ω: [{rsi_data.min():.2f}, {rsi_data.max():.2f}]")
                
                if 'open_time' in data_file.columns:
                    time_data = data_file['open_time'].dropna()
                    if len(time_data) > 0:
                        print(f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {time_data.min()} ‚Äî {time_data.max()}")
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
    
    # –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
    print(f"\nüöÄ –û–ë–£–ß–ï–ù–ò–ï –ù–ê –û–ë–™–ï–î–ò–ù–ï–ù–ù–´–• –î–ê–ù–ù–´–•:")
    print("-" * 50)
    
    trained_successfully = False
    predictor = None
    
    # –ü—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
    for data_file in data_files:
        try:
            if isinstance(data_file, str):
                print(f"\nüìä –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ñ–∞–π–ª–µ {Path(data_file).name}...")
                df = DataAdapter.load_csv(data_file)
            else:
                print(f"\nüìä –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
                df = data_file
            
            print(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {df.shape}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ rsi_volatility
            if 'rsi_volatility' not in df.columns:
                print(f"‚ùå –ù–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ rsi_volatility –≤ –¥–∞–Ω–Ω—ã—Ö")
                
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å rsi_volatility –∏–∑ close —Ü–µ–Ω—ã
                if 'close' in df.columns:
                    print("üîß –°–æ–∑–¥–∞–µ–º rsi_volatility –∏–∑ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è...")
                    import talib
                    df['rsi_volatility'] = talib.RSI(df['close'], timeperiod=14)
                    print(f"‚úÖ RSI volatility —Å–æ–∑–¥–∞–Ω")
                else:
                    continue
            
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ OHLCV —Ñ–æ—Ä–º–∞—Ç—É
            df_clean = DataAdapter.adapt_to_ohlcv(df)
            print(f"–î–∞–Ω–Ω—ã–µ –æ—á–∏—â–µ–Ω—ã: {df_clean.shape}")
            
            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            config = ModelConfig(
                model_type='catboost',
                test_size=0.15,  # –ú–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                cv_folds=5,
                catboost_params={
                    'iterations': 500,           # –ë–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                    'learning_rate': 0.03,       # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
                    'depth': 5,                  # –£–º–µ—Ä–µ–Ω–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
                    'l2_leaf_reg': 10,           # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                    'early_stopping_rounds': 50,
                    'verbose': False,
                    'random_seed': 42,
                    'eval_metric': 'RMSE',
                    'use_best_model': True
                }
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            predictor = RSIPredictor(config)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π
            models_dir = current_dir / "models"
            models_dir.mkdir(exist_ok=True)
            
            # –û–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            model_path = models_dir / "rsi_predictor_combined.pkl"
            print(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(df_clean)} —Å—Ç—Ä–æ–∫–∞—Ö...")
            
            metrics = predictor.train(df_clean, save_path=str(model_path))
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø:")
            for model_type, result in metrics.items():
                print(f"\n{model_type.upper()} –º–æ–¥–µ–ª—å:")
                if isinstance(result, dict):
                    for metric, value in result.items():
                        if isinstance(value, (int, float)):
                            if 'accuracy' in metric:
                                print(f"  {metric}: {value:.1f}%")
                            else:
                                print(f"  {metric}: {value:.4f}")
            
            # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            print(f"\nüîÆ –¢–ï–°–¢–û–í–û–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï:")
            result = predictor.predict(df_clean, return_confidence=True)
            
            print(f"üìÖ –î–∞—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {result.prediction_date.strftime('%d.%m.%Y %H:%M')}")
            print(f"üìä –¢–µ–∫—É—â–∏–π RSI: {result.current_rsi:.2f}")
            print(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π RSI: {result.predicted_rsi:.2f}")
            print(f"üìà –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {result.change:+.2f}")
            print(f"üé≤ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.1f}%")
            
            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            print(f"\nüí° –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:")
            if abs(result.change) < 1:
                print("üü° –ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ RSI - –±–æ–∫–æ–≤–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ")
            elif result.change > 3:
                print("üü¢ –°–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç RSI - –≤–æ–∑–º–æ–∂–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –≤–Ω–∏–∑")
            elif result.change < -3:
                print("üî¥ –°–∏–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ RSI - –≤–æ–∑–º–æ–∂–µ–Ω –æ—Ç—Å–∫–æ–∫ –≤–≤–µ—Ä—Ö")
            else:
                print("üîµ –£–º–µ—Ä–µ–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ RSI")
            
            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            print(f"\nüìä –¢–û–ü-10 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
            importance_df = predictor.get_feature_importance(10)
            if not importance_df.empty:
                for idx, row in importance_df.iterrows():
                    print(f"  {row['feature']:<30}: {row['importance']:.4f}")
            else:
                print("  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            
            trained_successfully = True
            break
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if trained_successfully:
        print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
        models_dir = current_dir / "models"
        if models_dir.exists():
            model_files = list(models_dir.glob("*.pkl"))
            print(f"\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
            for model_file in model_files:
                file_size = model_file.stat().st_size / (1024 * 1024)
                print(f"  üìÑ {model_file.name} ({file_size:.2f} MB)")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏")
        print("‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
        print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞")
        
        print(f"\nüîÑ –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
        print("  1. predictor = RSIPredictor()")
        print("  2. predictor.load('models/rsi_predictor_combined.pkl')")
        print("  3. result = predictor.predict(new_data, return_confidence=True)")
        
    else:
        print(f"\n‚ùå –û–ë–£–ß–ï–ù–ò–ï –ù–ï –£–î–ê–õ–û–°–¨")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("  - –ù–∞–ª–∏—á–∏–µ CSV —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ 'data' –∏–ª–∏ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ")
        print("  - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö (OHLC –∫–æ–ª–æ–Ω–∫–∏)")
        print("  - –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (–º–∏–Ω–∏–º—É–º 100 —Å—Ç—Ä–æ–∫)")

def demo_batch_prediction():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–∞–∫–µ—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
    
    print("\n" + "="*60)
    print("üîÑ –ü–ê–ö–ï–¢–ù–´–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –î–õ–Ø –í–°–ï–• –§–ê–ô–õ–û–í")
    print("="*60)
    
    try:
        from rsi_predictor import RSIPredictor
        from data_adapter import DataAdapter
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        model_path = current_dir / "models" / "rsi_predictor_combined.pkl"
        if not model_path.exists():
            print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ")
            return
        
        predictor = RSIPredictor()
        predictor.load(str(model_path))
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path.name}")
        
        # –ü–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        data_files = find_all_data_files()
        
        if not data_files:
            print("‚ùå –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        print(f"\nüîÆ –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {len(data_files)} —Ñ–∞–π–ª–æ–≤:")
        
        results = []
        
        for i, file_path in enumerate(data_files[:5], 1):  # –ü–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
            try:
                print(f"\n{i}. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è {file_path.name}:")
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                df = DataAdapter.load_csv(str(file_path))
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
                if len(df) < 30:
                    print(f"   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)} —Å—Ç—Ä–æ–∫")
                    continue
                
                df_clean = DataAdapter.adapt_to_ohlcv(df)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                result = predictor.predict(df_clean, return_confidence=True)
                
                print(f"   üìä –¢–µ–∫—É—â–∏–π RSI: {result.current_rsi:.2f}")
                print(f"   üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result.predicted_rsi:.2f}")
                print(f"   üìà –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {result.change:+.2f}")
                print(f"   üé≤ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.1f}%")
                
                results.append({
                    'file': file_path.name,
                    'current_rsi': result.current_rsi,
                    'predicted_rsi': result.predicted_rsi,
                    'change': result.change,
                    'confidence': result.confidence
                })
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
        
        # –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if results:
            print(f"\nüìã –°–í–û–î–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
            print("-" * 60)
            for result in results:
                direction = "üìà" if result['change'] > 1 else "üìâ" if result['change'] < -1 else "‚û°Ô∏è"
                print(f"{direction} {result['file']:<25} RSI: {result['current_rsi']:5.1f} ‚Üí {result['predicted_rsi']:5.1f} ({result['change']:+5.1f})")
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # –û—Å–Ω–æ–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    main()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–∞–∫–µ—Ç–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    demo_batch_prediction()