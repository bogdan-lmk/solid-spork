import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from xgboost.sklearn import XGBClassifier
from imblearn.over_sampling import SMOTE
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
import joblib
from sklearn.metrics import precision_recall_curve, roc_auc_score, average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('data.csv')

# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
X = df.drop(columns='target')
y = df['target']

# 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
#model = RandomForestClassifier(random_state=42, class_weight='balanced')
model = XGBClassifier(scale_pos_weight=8, use_label_encoder=False, eval_metric='logloss')

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
model.fit(X_train_res, y_train_res)

importance = model.feature_importances_
plt.figure(figsize=(8,5))
sns.barplot(x=importance, y=X.columns)
plt.title("XGBoost Feature Importance")
plt.show()

# 5. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
y_pred = model.predict(X_test)
print("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç:")
print(classification_report(y_test, y_pred))
print("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
print(confusion_matrix(y_test, y_pred))
probs = model.predict_proba(X_test)[:, 1]
print("ROC AUC:", roc_auc_score(y_test, probs))
print("Average Precision Score:", average_precision_score(y_test, probs))
# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É –ø–æ—Ä–æ–≥—É
y_pred_custom = (probs > 0.28)
print("\nüìä –û—Ç—á—ë—Ç –ø—Ä–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–º –ø–æ—Ä–æ–≥–µ:")
print(classification_report(y_test, y_pred_custom))
print("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
print(confusion_matrix(y_test, y_pred_custom))

probs = model.predict_proba(X_test)[:, 1]
precision, recall, thresholds = precision_recall_curve(y_test, probs)

# –ù–∞–π–¥–∏ F1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
best_threshold = thresholds[f1_scores.argmax()]
print("Best threshold:", best_threshold)

# 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
joblib.dump(model, 'trained_model.pkl')
print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'trained_model.pkl'")
# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = joblib.load("trained_model.pkl")

# –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)
initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
onnx_model = convert_sklearn(model, initial_types=initial_type)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open("trained_model.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())
